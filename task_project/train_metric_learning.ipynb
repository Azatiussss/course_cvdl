{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import random\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import linalg\n",
    "from pytorch_metric_learning.losses import ArcFaceLoss\n",
    "\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from albumentations import Compose, Normalize, Resize\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_metadata = pd.read_csv(\"/DATA/asaginbaev/DF20M/DF20M-train_metadata_PROD.csv\")\n",
    "print(len(train_metadata))\n",
    "\n",
    "test_metadata = pd.read_csv(\"/DATA/asaginbaev/DF20M/DF20M-public_test_metadata_PROD.csv\")\n",
    "print(len(test_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata['image_path'] = train_metadata.apply(lambda x: '/DATA/asaginbaev/DF20M/DF20M/' + x['image_path'].split('.')[0] + '.JPG', axis=1)\n",
    "test_metadata['image_path'] = test_metadata.apply(lambda x: '/DATA/asaginbaev/DF20M/DF20M/' + x['image_path'].split('.')[0] + '.JPG', axis=1)\n",
    "\n",
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обычный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = len(train_metadata['class_id'].unique())\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.df['image_path'].values[idx]\n",
    "        label = self.df['class_id'].values[idx]\n",
    "        image = cv2.imread(file_path)\n",
    "        \n",
    "        try:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        except:\n",
    "            print(file_path)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Датасет для triplet-loss обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С вероятностью 0.25 в качестве негативного выбирается простой пример, т.е. из произвольного другого класса, с вероятностью 0.75- тяжелый, т.е. из того же рода, но другого вида"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = len(train_metadata['class_id'].unique())\n",
    "\n",
    "class TLTrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.df['image_path'].values[idx]\n",
    "        label = self.df['class_id'].values[idx]\n",
    "        genus = self.df['genus'].values[idx]\n",
    "        is_hard_negative = np.random.choice([False, True], p=[0.25, 0.75])\n",
    "        if is_hard_negative:\n",
    "            negative_idx = np.random.choice(np.where(np.logical_and(self.df['genus'] == genus, \n",
    "                                                                self.df['class_id'] != label))[0])\n",
    "        else:\n",
    "            negative_idx = np.random.choice(np.where(self.df['class_id'] != label)[0])\n",
    "        positive_indices = np.where(self.df['class_id'] == label)[0]\n",
    "        positive_idx = np.random.choice(positive_indices[positive_indices != idx])\n",
    "        \n",
    "        \n",
    "        image_anchor = cv2.imread(file_path)\n",
    "        image_positive = cv2.imread(self.df['image_path'].values[positive_idx])\n",
    "        image_negative = cv2.imread(self.df['image_path'].values[negative_idx])\n",
    "        \n",
    "        try:\n",
    "            image_anchor = cv2.cvtColor(image_anchor, cv2.COLOR_BGR2RGB)\n",
    "            image_positive = cv2.cvtColor(image_positive, cv2.COLOR_BGR2RGB)\n",
    "            image_negative = cv2.cvtColor(image_negative, cv2.COLOR_BGR2RGB)      \n",
    "        except:\n",
    "            print(file_path)\n",
    "\n",
    "        if self.transform:\n",
    "            anchor_augmented = self.transform(image=image_anchor)\n",
    "            positive_augmented = self.transform(image=image_positive)\n",
    "            negative_augmented = self.transform(image=image_negative)\n",
    "            \n",
    "            image_anchor = anchor_augmented['image']\n",
    "            image_positive = positive_augmented['image']\n",
    "            image_negative = negative_augmented['image']\n",
    "            \n",
    "        \n",
    "        return image_anchor, image_positive, image_negative, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additive-margin-softmax лосс, реализация из https://github.com/Leethony/Additive-Margin-Softmax-Loss-Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdMSoftmaxLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.4):\n",
    "        '''\n",
    "        AM Softmax Loss\n",
    "        '''\n",
    "        super(AdMSoftmaxLoss, self).__init__()\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.fc = nn.Linear(in_features, out_features, bias=False)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        '''\n",
    "        input shape (N, in_features)\n",
    "        '''\n",
    "        \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        assert len(x) == len(labels)\n",
    "        assert torch.min(labels) >= 0\n",
    "        assert torch.max(labels) < self.out_features\n",
    "        \n",
    "        for W in self.fc.parameters():\n",
    "            W = F.normalize(W, dim=1)\n",
    "\n",
    "        x = F.normalize(x, dim=1)\n",
    "\n",
    "        wf = self.fc(x)\n",
    "        numerator = self.s * (torch.diagonal(wf.transpose(0, 1)[labels]) - self.m)\n",
    "        excl = torch.cat([torch.cat((wf[i, :y], wf[i, y+1:])).unsqueeze(0) for i, y in enumerate(labels)], dim=0)\n",
    "        denominator = torch.exp(numerator) + torch.sum(torch.exp(self.s * excl), dim=1)\n",
    "        L = numerator - torch.log(denominator)\n",
    "        return -torch.mean(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(architecture_name, target_size, pretrained = False):\n",
    "    net = timm.create_model(architecture_name, pretrained=pretrained)\n",
    "    net_cfg = net.default_cfg\n",
    "    last_layer = net_cfg['classifier']\n",
    "    num_ftrs = getattr(net, last_layer).in_features\n",
    "    setattr(net, last_layer, nn.Linear(num_ftrs, target_size))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "MODEL_NAME = 'vit_base_patch16_224'\n",
    "model = getModel(MODEL_NAME, N_CLASSES, pretrained=True)\n",
    "model_mean = list(model.default_cfg['mean'])\n",
    "model_std = list(model.default_cfg['std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('DF20M-ViT_base_patch16_224_best_accuracy.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH, HEIGHT = 224, 224\n",
    "\n",
    "from albumentations import RandomCrop, HorizontalFlip, VerticalFlip, RandomBrightnessContrast, CenterCrop, PadIfNeeded, RandomResizedCrop\n",
    "\n",
    "def get_transforms(*, data):\n",
    "    assert data in ('train', 'valid')\n",
    "\n",
    "    if data == 'train':\n",
    "        return Compose([\n",
    "            RandomResizedCrop(WIDTH, HEIGHT, scale=(0.8, 1.0)),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            RandomBrightnessContrast(p=0.2),\n",
    "            Normalize(mean=model_mean, std=model_std),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    elif data == 'valid':\n",
    "        return Compose([\n",
    "            Resize(WIDTH, HEIGHT),\n",
    "            Normalize(mean=model_mean, std=model_std),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TLTrainDataset(train_metadata, transform=get_transforms(data='train'))\n",
    "valid_dataset = TLTrainDataset(test_metadata, transform=get_transforms(data='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(train_metadata, transform=get_transforms(data='train'))\n",
    "valid_dataset = TrainDataset(test_metadata, transform=get_transforms(data='valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "WORKERS = 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели с ArcFace лоссом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, accuracy_score, top_k_accuracy_score\n",
    "\n",
    "wandb.init(project=\"danish-fungi\")\n",
    "\n",
    "n_epochs = EPOCHS\n",
    "lr = 1e-3\n",
    "    \n",
    "model.to(device)\n",
    "    \n",
    "optimizer = SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=1, verbose=True, eps=1e-6)\n",
    "    \n",
    "criterion_classification = nn.CrossEntropyLoss()\n",
    "criterion_metrics = ArcFaceLoss(182, 197 * 768, margin=5.7, scale=8).to(device)\n",
    "\n",
    "optimizer_criterion = SGD(criterion_metrics.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler_criterion = ReduceLROnPlateau(optimizer_criterion, 'min', factor=0.9, patience=1, verbose=True, eps=1e-6)\n",
    "    \n",
    "for epoch in range(n_epochs):\n",
    "        \n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_criterion.zero_grad()\n",
    "    \n",
    "\n",
    "    for i, (images, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        y_features = model.forward_features(images)\n",
    "        loss_metrics = criterion_metrics(torch.flatten(y_features, start_dim=1), labels)\n",
    "        \n",
    "        loss = loss_metrics\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_criterion.step()\n",
    "        optimizer_criterion.zero_grad()\n",
    "        wandb.log({\n",
    "                   \"train/arcface loss\": loss_metrics.item(), \n",
    "                   \"train/loss total\": loss.item(),\n",
    "                   \"epoch\": epoch,})\n",
    "\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "    wandb.log({\"train/avg loss\": avg_loss,\n",
    "                \"epoch\": epoch,})\n",
    "    model.eval()\n",
    "    avg_val_loss = 0.\n",
    "    preds = np.zeros((len(valid_dataset)))\n",
    "    preds_raw = []\n",
    "\n",
    "    for i, (images, labels) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "            \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "            y_features = model.forward_features(images)\n",
    "            \n",
    "        preds[i * BATCH_SIZE: (i+1) * BATCH_SIZE] = y_preds.argmax(1).to('cpu').numpy()\n",
    "        preds_raw.extend(y_preds.to('cpu').numpy())\n",
    "\n",
    "        loss_metrics = criterion_metrics(torch.flatten(y_features, start_dim=1), labels)\n",
    "        \n",
    "        loss = loss_metrics\n",
    "        wandb.log({\n",
    "                   \"val/arcface loss\": loss_metrics.item(), \n",
    "                   \"val/loss total\": loss.item(),\n",
    "                   \"epoch\": epoch,})\n",
    "        avg_val_loss += loss.item() / len(valid_loader)\n",
    "    wandb.log({\"val/avg loss\": avg_val_loss,\n",
    "                \"epoch\": epoch,})     \n",
    "    scheduler.step(avg_val_loss)\n",
    "    scheduler_criterion.step(avg_val_loss)\n",
    "    \n",
    "            \n",
    "    score = f1_score(test_metadata['class_id'], preds, average='macro')\n",
    "    accuracy = accuracy_score(test_metadata['class_id'], preds)\n",
    "    recall_3 = top_k_accuracy_score(test_metadata['class_id'], preds_raw, k=3)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    wandb.log({\"val/f1-score\": score, \n",
    "               \"val/accuracy\": accuracy, \n",
    "               \"val/top-3 accuracy\": recall_3,\n",
    "               \"epoch\": epoch,\n",
    "               \"time elapsed\": elapsed})\n",
    "    \n",
    "    if (epoch - 9) % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'checkpoints/VIT_arcface_epoch_{epoch}.pth')\n",
    "        torch.save(criterion_metrics.state_dict(), f'checkpoints/arcface_epoch_{epoch}.pth')        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение с additive margin softmax лоссом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, accuracy_score, top_k_accuracy_score\n",
    "\n",
    "wandb.init(project=\"danish-fungi\")\n",
    "\n",
    "n_epochs = EPOCHS\n",
    "lr = 1e-4\n",
    "    \n",
    "model.to(device)\n",
    "    \n",
    "optimizer = SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=1, verbose=True, eps=1e-6)\n",
    "    \n",
    "criterion_classification = nn.CrossEntropyLoss()\n",
    "criterion_metrics = AdMSoftmaxLoss(768 * 197, 182, s=8, m=0.1).to(device)\n",
    "\n",
    "optimizer_criterion = SGD(criterion_metrics.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler_criterion = ReduceLROnPlateau(optimizer_criterion, 'min', factor=0.9, patience=1, verbose=True, eps=1e-6)\n",
    "    \n",
    "for epoch in range(n_epochs):\n",
    "        \n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_criterion.zero_grad()\n",
    "    \n",
    "\n",
    "    for i, (images, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        y_features = model.forward_features(images)\n",
    "        loss_metrics = criterion_metrics(y_features, labels)\n",
    "        \n",
    "        loss = loss_metrics\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_criterion.step()\n",
    "        optimizer_criterion.zero_grad()\n",
    "        wandb.log({\n",
    "                   \"train/am softmax loss\": loss_metrics.item(), \n",
    "                   \"train/loss total\": loss.item(),\n",
    "                   \"epoch\": epoch,})\n",
    "\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "    wandb.log({\"train/avg loss\": avg_loss,\n",
    "                \"epoch\": epoch,})\n",
    "    model.eval()\n",
    "    avg_val_loss = 0.\n",
    "    preds = np.zeros((len(valid_dataset)))\n",
    "    preds_raw = []\n",
    "\n",
    "    for i, (images, labels) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "            \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "            y_features = model.forward_features(images)\n",
    "            \n",
    "        preds[i * BATCH_SIZE: (i+1) * BATCH_SIZE] = y_preds.argmax(1).to('cpu').numpy()\n",
    "        preds_raw.extend(y_preds.to('cpu').numpy())\n",
    "\n",
    "        loss_metrics = criterion_metrics(y_features, labels)\n",
    "        \n",
    "        loss = loss_metrics\n",
    "        wandb.log({\n",
    "                   \"val/am softmax loss\": loss_metrics.item(), \n",
    "                   \"val/loss total\": loss.item(),\n",
    "                   \"epoch\": epoch,})\n",
    "        avg_val_loss += loss.item() / len(valid_loader)\n",
    "    wandb.log({\"val/avg loss\": avg_val_loss,\n",
    "                \"epoch\": epoch,})     \n",
    "    scheduler.step(avg_val_loss)\n",
    "    scheduler_criterion.step(avg_val_loss)\n",
    "    \n",
    "            \n",
    "    score = f1_score(test_metadata['class_id'], preds, average='macro')\n",
    "    accuracy = accuracy_score(test_metadata['class_id'], preds)\n",
    "    recall_3 = top_k_accuracy_score(test_metadata['class_id'], preds_raw, k=3)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    wandb.log({\"val/f1-score\": score, \n",
    "               \"val/accuracy\": accuracy, \n",
    "               \"val/top-3 accuracy\": recall_3,\n",
    "               \"epoch\": epoch,\n",
    "               \"time elapsed\": elapsed})\n",
    "    \n",
    "    if (epoch - 9) % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'checkpoints/VIT_am_softmax_epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение с triplet лоссом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, accuracy_score, top_k_accuracy_score\n",
    "\n",
    "wandb.init(project=\"danish-fungi\")\n",
    "\n",
    "n_epochs = EPOCHS\n",
    "lr = 1e-4\n",
    "    \n",
    "model.to(device)\n",
    "    \n",
    "optimizer = SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=1, verbose=True, eps=1e-6)\n",
    "    \n",
    "criterion_classification = nn.CrossEntropyLoss()\n",
    "criterion_metrics = nn.TripletMarginLoss()\n",
    "    \n",
    "for epoch in range(n_epochs):\n",
    "        \n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (images, images_positive, images_negative, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "\n",
    "        images = images.to(device)\n",
    "        images_positive = images_positive.to(device)\n",
    "        images_negative = images_negative.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        y_preds = model(images)\n",
    "        y_features = model.forward_features(images)\n",
    "        y_features_positive = model.forward_features(images_positive)\n",
    "        y_features_negative = model.forward_features(images_negative)\n",
    "        loss_metrics = criterion_metrics(y_features, y_features_positive, y_features_negative)\n",
    "        \n",
    "        loss = loss_metrics\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        wandb.log({\"train/triplet loss\": loss_metrics.item(), \n",
    "                   \"train/loss total\": loss.item(),\n",
    "                   \"epoch\": epoch,})\n",
    "\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "    wandb.log({\"train/avg loss\": avg_loss,\n",
    "                \"epoch\": epoch,})\n",
    "    model.eval()\n",
    "    avg_val_loss = 0.\n",
    "    preds = np.zeros((len(valid_dataset)))\n",
    "    preds_raw = []\n",
    "\n",
    "    for i, (images, images_positive, images_negative, labels) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "            \n",
    "        images = images.to(device)\n",
    "        images_positive = images_positive.to(device)\n",
    "        images_negative = images_negative.to(device)\n",
    "        labels = labels.to(device)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "            y_features = model.forward_features(images)\n",
    "            y_features_positive = model.forward_features(images_positive)\n",
    "            y_features_negative = model.forward_features(images_negative)\n",
    "            \n",
    "        preds[i * BATCH_SIZE: (i+1) * BATCH_SIZE] = y_preds.argmax(1).to('cpu').numpy()\n",
    "        preds_raw.extend(y_preds.to('cpu').numpy())\n",
    "\n",
    "        loss_metrics = criterion_metrics(y_features, y_features_positive, y_features_negative)\n",
    "        \n",
    "        loss = loss_metrics\n",
    "        wandb.log({\"val/triplet loss\": loss_metrics.item(), \n",
    "                   \"val/loss total\": loss.item(),\n",
    "                   \"epoch\": epoch,})\n",
    "        avg_val_loss += loss.item() / len(valid_loader)\n",
    "    wandb.log({\"val/avg loss\": avg_val_loss,\n",
    "                \"epoch\": epoch,})     \n",
    "    scheduler.step(avg_val_loss)\n",
    "            \n",
    "    score = f1_score(test_metadata['class_id'], preds, average='macro')\n",
    "    accuracy = accuracy_score(test_metadata['class_id'], preds)\n",
    "    recall_3 = top_k_accuracy_score(test_metadata['class_id'], preds_raw, k=3)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    wandb.log({\"val/f1-score\": score, \n",
    "               \"val/accuracy\": accuracy, \n",
    "               \"val/top-3 accuracy\": recall_3,\n",
    "               \"epoch\": epoch,\n",
    "               \"time elapsed\": elapsed})\n",
    "    \n",
    "    torch.save(model.state_dict(), f'checkpoints/VIT_triplet_epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Замораживаем слои трансформера, оставляя обучаться только классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.head.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score, accuracy_score, top_k_accuracy_score\n",
    "\n",
    "wandb.init(project=\"danish-fungi\")\n",
    "\n",
    "EPOCHS = 50\n",
    "n_epochs = EPOCHS\n",
    "lr = 1e-3\n",
    "    \n",
    "model.to(device)\n",
    "    \n",
    "optimizer = SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=1, verbose=True, eps=1e-6)\n",
    "    \n",
    "criterion_classification = nn.CrossEntropyLoss()\n",
    "    \n",
    "for epoch in range(n_epochs):\n",
    "        \n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (images, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        y_preds = model(images)\n",
    "        loss_classification = criterion_classification(y_preds, labels)\n",
    "        \n",
    "        loss = loss_classification\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        wandb.log({\"train/cross-entropy\": loss_classification.item(), \n",
    "                   \"train/loss total\": loss.item(),\n",
    "                   \"epoch\": epoch,})\n",
    "\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "    wandb.log({\"train/avg loss\": avg_loss,\n",
    "                \"epoch\": epoch,})\n",
    "    model.eval()\n",
    "    avg_val_loss = 0.\n",
    "    preds = np.zeros((len(valid_dataset)))\n",
    "    preds_raw = []\n",
    "\n",
    "    for i, (images, labels) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "            \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "\n",
    "        preds[i * BATCH_SIZE: (i+1) * BATCH_SIZE] = y_preds.argmax(1).to('cpu').numpy()\n",
    "        preds_raw.extend(y_preds.to('cpu').numpy())\n",
    "\n",
    "        loss_classification = criterion_classification(y_preds, labels)\n",
    "        \n",
    "        loss = loss_classification\n",
    "        wandb.log({\"val/cross-entropy\": loss_classification.item(), \n",
    "                   \"val/loss total\": loss.item(),\n",
    "                   \"epoch\": epoch,})\n",
    "        avg_val_loss += loss.item() / len(valid_loader)\n",
    "    wandb.log({\"val/avg loss\": avg_val_loss,\n",
    "                \"epoch\": epoch,})    \n",
    "    scheduler.step(avg_val_loss)\n",
    "            \n",
    "    score = f1_score(test_metadata['class_id'], preds, average='macro')\n",
    "    accuracy = accuracy_score(test_metadata['class_id'], preds)\n",
    "    recall_3 = top_k_accuracy_score(test_metadata['class_id'], preds_raw, k=3)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    wandb.log({\"val/f1-score\": score, \n",
    "               \"val/accuracy\": accuracy, \n",
    "               \"val/top-3 accuracy\": recall_3,\n",
    "               \"epoch\": epoch,\n",
    "               \"time elapsed\": elapsed})\n",
    "    \n",
    "    if (epoch - 9) % 10 == 0:\n",
    "        torch.save(model.state_dict(), f'checkpoints/VIT_classifier_epoch_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсчет финальных метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.zeros((len(valid_dataset)))\n",
    "preds_raw = []\n",
    "\n",
    "for i, (images, labels) in enumerate(valid_loader):\n",
    "            \n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        y_preds = model(images)\n",
    "            \n",
    "    preds[i * BATCH_SIZE: (i+1) * BATCH_SIZE] = y_preds.argmax(1).to('cpu').numpy()\n",
    "    preds_raw.extend(y_preds.to('cpu').numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = sklearn.metrics.f1_score(test_metadata['class_id'], preds, average='macro')\n",
    "accuracy = sklearn.metrics.accuracy_score(test_metadata['class_id'], preds)\n",
    "recall_3 = sklearn.metrics.top_k_accuracy_score(test_metadata['class_id'], preds_raw, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попытка обучить на эмбеддингах классификатор на основе k-средних "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import (NeighborhoodComponentsAnalysis, KNeighborsClassifier)\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "classes = []\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        images = images.to(device)\n",
    "        y_features = model.forward_features(images)\n",
    "        embeddings.append(torch.flatten(y_features, start_dim=1).cpu().detach().numpy())\n",
    "        classes.append(labels.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_concat = np.concatenate(embeddings, axis=0)\n",
    "classes_concat =  np.concatenate(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca = NeighborhoodComponentsAnalysis(verbose=1, n_components=128)\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "nca_pipe = Pipeline([('nca', nca), ('knn', knn)], verbose=True)\n",
    "nca_pipe.fit(embeddings_concat[:, -768 * 2:], classes_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_test = []\n",
    "classes_test = []\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in tqdm(enumerate(valid_loader), total=len(valid_loader)):\n",
    "        images = images.to(device)\n",
    "        y_features = model.forward_features(images)\n",
    "        embeddings_test.append(torch.flatten(y_features, start_dim=1).cpu().detach().numpy())\n",
    "        classes_test.append(labels.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_test_concat = np.concatenate(embeddings_test, axis=0)\n",
    "classes_test_concat =  np.concatenate(classes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nca_pipe.score(embeddings_test_concat[:, -768 * 2:], classes_test_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
